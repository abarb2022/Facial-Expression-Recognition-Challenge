{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abarb2022/Facial-Expression-Recognition-Challenge/blob/main/simple_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/yEXkEUqK52Q"
      ],
      "metadata": {
        "id": "RfYmOIDroDLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the kaggle python library"
      ],
      "metadata": {
        "id": "7lvdgeEMgCoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "4bbab928-17fb-4bc2-8d3d-256950292226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "rw0DfSAggHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGineQt7dErh",
        "outputId": "280fb0e2-9003-4232-ed96-c5ddc1366f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "Rvmi3WbigOmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vhywUxLXgjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZTkKggcylXfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to copy the kaggle API credentials to the temporary location... (I recommend placing it on your Google Drive)"
      ],
      "metadata": {
        "id": "rKv_7jNggXv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "DD56NrWmlb5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "p3N4it0xrFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IQq6ZMyTrEfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the file permissions to read/write to the owner only"
      ],
      "metadata": {
        "id": "p3dHJgtLehrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7ncAtrq2lg5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitions and Datasets are the two types of Kaggle data**"
      ],
      "metadata": {
        "id": "Rb3Zm9VMlu3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download competition data**\n",
        "\n",
        "If you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition."
      ],
      "metadata": {
        "id": "OrdSFfGjl3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0yNdtoRln8A",
        "outputId": "84f8484e-324b-42e3-9163-4fd6dea1424c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 89% 254M/285M [00:01<00:00, 126MB/s] \n",
            "100% 285M/285M [00:01<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view."
      ],
      "metadata": {
        "id": "fRmXZnHghNAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs9oVnNoziL",
        "outputId": "9bdc5997-2236-47db-ef9d-145bed3506e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnza6oWyTVMp",
        "outputId": "f0db9bee-d7e0-4181-8cf6-09a3717c5dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv('icml_face_data.csv')\n",
        "\n",
        "# # Split data according to original splits\n",
        "# train_df = data_df[data_df[' Usage'] == 'Training']\n",
        "# val_df = data_df[data_df[' Usage'] == 'PublicTest']\n",
        "# test_df = data_df[data_df[' Usage'] == 'PrivateTest']\n",
        "\n",
        " if ' Usage' not in data_df.columns:\n",
        "        print(\"Warning: 'Usage' column not found in dataset\")\n",
        "        return data_df\n",
        "\n",
        "    # Split data and remove 'Usage' column\n",
        "    train_df = data_df[data_df[' Usage'] == 'Training'].drop(columns=[' Usage'])\n",
        "    val_df = data_df[data_df[' Usage'] == 'PublicTest'].drop(columns=[' Usage'])\n",
        "    test_df = data_df[data_df[' Usage'] == 'PrivateTest'].drop(columns=[' Usage'])"
      ],
      "metadata": {
        "id": "EGCd9MjcTVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleFERDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx][' pixels']\n",
        "        pixels = np.array([int(pixel) for pixel in pixels.split()], dtype=np.float32)\n",
        "        image = pixels.reshape(1, 48, 48)  # 1x48x48 (channel first)\n",
        "\n",
        "        # Simple normalization [0, 255] -> [0, 1]\n",
        "        image = image / 255.0\n",
        "\n",
        "        label = self.data.iloc[idx]['emotion']\n",
        "        return torch.FloatTensor(image), torch.LongTensor([label])"
      ],
      "metadata": {
        "id": "ZucYei_vTWND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Very simple CNN model\n",
        "class SimpleEmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(SimpleEmotionCNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # 1x48x48 -> 32x24x24\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 32x24x24 -> 64x12x12\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 64x12x12 -> 128x6x6\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 6 * 6, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7cLPEBziLySI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SimpleFERDataset(train_df)\n",
        "val_dataset = SimpleFERDataset(val_df)\n",
        "test_dataset = SimpleFERDataset(test_df)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "e424zzYIa5GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize model, loss, and optimizer\n",
        "# model = SimpleEmotionCNN(num_classes=7).to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "xoosyaxCa5EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def train_with_history(model, train_loader, val_loader, num_epochs=15, lr=0.001):\n",
        "    history = defaultdict(list)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_weights = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store batch metrics every 50 batches\n",
        "            if batch_idx % 50 == 0:\n",
        "                history['batch'].append(epoch * len(train_loader) + batch_idx)\n",
        "                history['train_batch_loss'].append(loss.item())\n",
        "                history['train_batch_acc'].append((predicted == labels).sum().item() / labels.size(0))\n",
        "\n",
        "        # Store epoch metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device).squeeze()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_weights = model.state_dict().copy()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Store confusion matrix data\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    history['confusion_matrix'] = {\n",
        "        'true': true_labels,\n",
        "        'pred': pred_labels,\n",
        "        'class_names': [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "    }\n",
        "\n",
        "    # Restore best model weights\n",
        "    if best_model_weights:\n",
        "        model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "8SnDs4uSapGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': 15,\n",
        "    'architecture': 'SimpleCNN',\n",
        "    'optimizer': 'Adam'\n",
        "}\n",
        "\n",
        "# Initialize and train model\n",
        "model = SimpleEmotionCNN(num_classes=7).to(device)\n",
        "trained_model, training_history = train_with_history(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=config['epochs'],\n",
        "    lr=config['learning_rate']\n",
        ")\n",
        "\n",
        "# Test evaluation\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "test_acc = evaluate(trained_model, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "training_history['test_acc'] = test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acVOl3Kca4_c",
        "outputId": "268df112-ea94-45ab-faea-20f5cbe445fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15:\n",
            "Train Loss: 1.7107 | Train Acc: 0.3087\n",
            "Val Loss: 1.5310 | Val Acc: 0.4048\n",
            "Epoch 2/15:\n",
            "Train Loss: 1.4951 | Train Acc: 0.4212\n",
            "Val Loss: 1.4055 | Val Acc: 0.4581\n",
            "Epoch 3/15:\n",
            "Train Loss: 1.3763 | Train Acc: 0.4716\n",
            "Val Loss: 1.3255 | Val Acc: 0.4896\n",
            "Epoch 4/15:\n",
            "Train Loss: 1.2906 | Train Acc: 0.5086\n",
            "Val Loss: 1.2865 | Val Acc: 0.5018\n",
            "Epoch 5/15:\n",
            "Train Loss: 1.2162 | Train Acc: 0.5382\n",
            "Val Loss: 1.2277 | Val Acc: 0.5286\n",
            "Epoch 6/15:\n",
            "Train Loss: 1.1523 | Train Acc: 0.5636\n",
            "Val Loss: 1.2091 | Val Acc: 0.5378\n",
            "Epoch 7/15:\n",
            "Train Loss: 1.0970 | Train Acc: 0.5853\n",
            "Val Loss: 1.1896 | Val Acc: 0.5478\n",
            "Epoch 8/15:\n",
            "Train Loss: 1.0336 | Train Acc: 0.6120\n",
            "Val Loss: 1.1929 | Val Acc: 0.5481\n",
            "Epoch 9/15:\n",
            "Train Loss: 0.9743 | Train Acc: 0.6318\n",
            "Val Loss: 1.1955 | Val Acc: 0.5528\n",
            "Epoch 10/15:\n",
            "Train Loss: 0.9183 | Train Acc: 0.6569\n",
            "Val Loss: 1.2093 | Val Acc: 0.5598\n",
            "Epoch 11/15:\n",
            "Train Loss: 0.8529 | Train Acc: 0.6830\n",
            "Val Loss: 1.2456 | Val Acc: 0.5539\n",
            "Epoch 12/15:\n",
            "Train Loss: 0.8013 | Train Acc: 0.7003\n",
            "Val Loss: 1.2544 | Val Acc: 0.5620\n",
            "Epoch 13/15:\n",
            "Train Loss: 0.7342 | Train Acc: 0.7245\n",
            "Val Loss: 1.2956 | Val Acc: 0.5556\n",
            "Epoch 14/15:\n",
            "Train Loss: 0.6898 | Train Acc: 0.7422\n",
            "Val Loss: 1.3586 | Val Acc: 0.5626\n",
            "Epoch 15/15:\n",
            "Train Loss: 0.6451 | Train Acc: 0.7599\n",
            "Val Loss: 1.3993 | Val Acc: 0.5570\n",
            "Test Accuracy: 0.5706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()  # Follow the link to authenticate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "0gm-VCSngMWV",
        "outputId": "74e4625d-d2d5-4ede-be0b-5836af9126d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabarb2022\u001b[0m (\u001b[33mabarb2022-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wandb.init(project=\"facial-expression-recognition\", config=config)\n",
        "\n",
        "# Log hyperparameters\n",
        "wandb.config.update(config)\n",
        "\n",
        "for epoch in range(len(training_history['epoch'])):\n",
        "    wandb.log({\n",
        "        'epoch': training_history['epoch'][epoch],\n",
        "        'train_loss': training_history['train_loss'][epoch],\n",
        "        'train_acc': training_history['train_acc'][epoch],\n",
        "        'val_loss': training_history['val_loss'][epoch],\n",
        "        'val_acc': training_history['val_acc'][epoch]\n",
        "    })\n",
        "\n",
        "# Log batch metrics (sampled to avoid too many points)\n",
        "if 'batch' in training_history:\n",
        "    batch_indices = np.linspace(0, len(training_history['batch'])-1, 1000, dtype=int)\n",
        "    for idx in batch_indices:\n",
        "        wandb.log({\n",
        "            'batch': training_history['batch'][idx],\n",
        "            'train_batch_loss': training_history['train_batch_loss'][idx],\n",
        "            'train_batch_acc': training_history['train_batch_acc'][idx]\n",
        "        }, commit=False)\n",
        "\n",
        "# Log confusion matrix\n",
        "if 'confusion_matrix' in training_history:\n",
        "    cm_data = training_history['confusion_matrix']\n",
        "    wandb.log({\n",
        "        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "            y_true=cm_data['true'],\n",
        "            preds=cm_data['pred'],\n",
        "            class_names=cm_data['class_names']\n",
        "        )\n",
        "    })\n",
        "\n",
        "# Log model architecture\n",
        "wandb.watch(model, log='all', log_freq=100, log_graph=True)\n",
        "\n",
        "# Log final metrics\n",
        "wandb.summary['best_val_acc'] = max(training_history['val_acc'])\n",
        "wandb.summary['final_train_acc'] = training_history['train_acc'][-1]\n",
        "\n",
        "# Save model\n",
        "torch.save(trained_model.state_dict(), 'model_weights.pth')\n",
        "wandb.save('model_weights.pth')\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "YiZHd4NwQ9tE",
        "outputId": "9c7302e8-3b68-4e0b-b831-4b66fcc4d51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worldly-energy-1</strong> at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/btbolfck' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/btbolfck</a><br> View project at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250606_231408-btbolfck/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250606_231859-j8ev6ww4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/j8ev6ww4' target=\"_blank\">sage-gorge-2</a></strong> to <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/j8ev6ww4' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/j8ev6ww4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_batch_acc</td><td>▁</td></tr><tr><td>train_batch_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▅▅▆▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▁▁▁▁▁▂▂▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>6686</td></tr><tr><td>best_val_acc</td><td>0.56255</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>final_train_acc</td><td>0.75987</td></tr><tr><td>train_acc</td><td>0.75987</td></tr><tr><td>train_batch_acc</td><td>0.78125</td></tr><tr><td>train_batch_loss</td><td>0.66259</td></tr><tr><td>train_loss</td><td>0.64513</td></tr><tr><td>val_acc</td><td>0.55698</td></tr><tr><td>val_loss</td><td>1.39934</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sage-gorge-2</strong> at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/j8ev6ww4' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition/runs/j8ev6ww4</a><br> View project at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/facial-expression-recognition</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250606_231859-j8ev6ww4/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5uQIfy1M6PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
