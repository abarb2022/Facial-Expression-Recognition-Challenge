{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAlvYJrkFLIv",
        "outputId": "eae278d6-390a-4b73-9656-7446297104c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNi0TZBlFLIw",
        "outputId": "5689c6f4-659a-4b91-bd0d-8e1090347cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "trusted": true,
        "id": "qOibmuhCFLIx"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fq2I1-3FLIx",
        "outputId": "2d29f5bf-3d56-4514-aefc-9bd5a0232cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 91% 260M/285M [00:00<00:00, 571MB/s]\n",
            "100% 285M/285M [00:00<00:00, 607MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8fLYyLJFYfF",
        "outputId": "ea857898-4228-46aa-f27f-dd459a6f5a46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# BATCH_SIZE = 128\n",
        "# EPOCHS = 50\n",
        "# LEARNING_RATE = 0.001\n",
        "# IMAGE_SIZE = 48\n",
        "# NUM_CLASSES = 7\n",
        "# # MODIFIED: Path to the single dataset file\n",
        "# DATA_CSV_PATH = 'icml_face_data.csv'\n"
      ],
      "metadata": {
        "id": "0mmKrPz6Iumr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv('icml_face_data.csv')\n",
        "\n",
        "# # Split data according to original splits\n",
        "# train_df = data_df[data_df[' Usage'] == 'Training']\n",
        "# val_df = data_df[data_df[' Usage'] == 'PublicTest']\n",
        "# test_df = data_df[data_df[' Usage'] == 'PrivateTest']\n",
        "\n",
        "if ' Usage' not in data_df.columns:\n",
        "      print(\"Warning: 'Usage' column not found in dataset\")\n",
        "\n",
        "    # Split data and remove 'Usage' column\n",
        "train_df = data_df[data_df[' Usage'] == 'Training'].drop(columns=[' Usage'])\n",
        "val_df = data_df[data_df[' Usage'] == 'PublicTest'].drop(columns=[' Usage'])\n",
        "test_df = data_df[data_df[' Usage'] == 'PrivateTest'].drop(columns=[' Usage'])"
      ],
      "metadata": {
        "id": "zKzsCNvvFwFM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FERDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx][' pixels']\n",
        "        pixels = np.array([int(pixel) for pixel in pixels.split()], dtype=np.float32)\n",
        "        image = pixels.reshape(48, 48, 1)  # HWC format for transforms\n",
        "        label = self.data.iloc[idx]['emotion']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Convert to tensor and normalize if no augmentation\n",
        "            image = torch.FloatTensor(image.transpose(2, 0, 1)) / 255.0\n",
        "\n",
        "        return image, torch.LongTensor([label])"
      ],
      "metadata": {
        "id": "Jz8GxMZDFwCt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BetterFERModel(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(BetterFERModel, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.4),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((3, 3)),\n",
        "            nn.Dropout2d(0.4),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3 * 3, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "21dHgNtPFwAY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FERDataset(train_df)\n",
        "val_dataset = FERDataset(val_df)\n",
        "test_dataset = FERDataset(test_df)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "1IB_XrNvFv7g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def train_with_history(model, train_loader, val_loader, num_epochs=15, lr=0.001):\n",
        "    history = defaultdict(list)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Added weight decay\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=False)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_weights = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store batch metrics every 50 batches\n",
        "            if batch_idx % 50 == 0:\n",
        "                history['batch'].append(epoch * len(train_loader) + batch_idx)\n",
        "                history['train_batch_loss'].append(loss.item())\n",
        "                history['train_batch_acc'].append((predicted == labels).sum().item() / labels.size(0))\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device).squeeze()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_weights = model.state_dict().copy()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "        print(f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    # Store confusion matrix data\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    history['confusion_matrix'] = {\n",
        "        'true': true_labels,\n",
        "        'pred': pred_labels,\n",
        "        'class_names': [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "    }\n",
        "\n",
        "    # Restore best model weights\n",
        "    if best_model_weights:\n",
        "        model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "7zpEInToFv4_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BetterFERModel(num_classes=7).to(device)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomResizedCrop(48, scale=(0.8,1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "\n",
        "# Train with history tracking\n",
        "trained_model, training_history = train_with_history(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    lr=0.001\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei2HHOLlP82V",
        "outputId": "02881d91-87a9-4ddb-8e1e-8354e74f47a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20:\n",
            "Train Loss: 1.8382 | Train Acc: 0.2343\n",
            "Val Loss: 1.7703 | Val Acc: 0.2611\n",
            "LR: 0.001000\n",
            "Epoch 2/20:\n",
            "Train Loss: 1.7859 | Train Acc: 0.2565\n",
            "Val Loss: 1.7129 | Val Acc: 0.2979\n",
            "LR: 0.001000\n",
            "Epoch 3/20:\n",
            "Train Loss: 1.6116 | Train Acc: 0.3565\n",
            "Val Loss: 1.4378 | Val Acc: 0.4380\n",
            "LR: 0.001000\n",
            "Epoch 4/20:\n",
            "Train Loss: 1.4132 | Train Acc: 0.4488\n",
            "Val Loss: 1.2967 | Val Acc: 0.4882\n",
            "LR: 0.001000\n",
            "Epoch 5/20:\n",
            "Train Loss: 1.3274 | Train Acc: 0.4836\n",
            "Val Loss: 1.2278 | Val Acc: 0.5191\n",
            "LR: 0.001000\n",
            "Epoch 6/20:\n",
            "Train Loss: 1.2781 | Train Acc: 0.5028\n",
            "Val Loss: 1.1994 | Val Acc: 0.5347\n",
            "LR: 0.001000\n",
            "Epoch 7/20:\n",
            "Train Loss: 1.2370 | Train Acc: 0.5236\n",
            "Val Loss: 1.1648 | Val Acc: 0.5489\n",
            "LR: 0.001000\n",
            "Epoch 8/20:\n",
            "Train Loss: 1.1989 | Train Acc: 0.5431\n",
            "Val Loss: 1.1391 | Val Acc: 0.5723\n",
            "LR: 0.001000\n",
            "Epoch 9/20:\n",
            "Train Loss: 1.1754 | Train Acc: 0.5508\n",
            "Val Loss: 1.1142 | Val Acc: 0.5726\n",
            "LR: 0.001000\n",
            "Epoch 10/20:\n",
            "Train Loss: 1.1469 | Train Acc: 0.5648\n",
            "Val Loss: 1.1144 | Val Acc: 0.5804\n",
            "LR: 0.001000\n",
            "Epoch 11/20:\n",
            "Train Loss: 1.1248 | Train Acc: 0.5786\n",
            "Val Loss: 1.1006 | Val Acc: 0.5770\n",
            "LR: 0.001000\n",
            "Epoch 12/20:\n",
            "Train Loss: 1.1057 | Train Acc: 0.5845\n",
            "Val Loss: 1.1064 | Val Acc: 0.5765\n",
            "LR: 0.001000\n",
            "Epoch 13/20:\n",
            "Train Loss: 1.0829 | Train Acc: 0.5941\n",
            "Val Loss: 1.0632 | Val Acc: 0.5977\n",
            "LR: 0.001000\n",
            "Epoch 14/20:\n",
            "Train Loss: 1.0697 | Train Acc: 0.5971\n",
            "Val Loss: 1.0424 | Val Acc: 0.6055\n",
            "LR: 0.001000\n",
            "Epoch 15/20:\n",
            "Train Loss: 1.0496 | Train Acc: 0.6082\n",
            "Val Loss: 1.0521 | Val Acc: 0.6021\n",
            "LR: 0.001000\n",
            "Epoch 16/20:\n",
            "Train Loss: 1.0376 | Train Acc: 0.6137\n",
            "Val Loss: 1.0310 | Val Acc: 0.6180\n",
            "LR: 0.001000\n",
            "Epoch 17/20:\n",
            "Train Loss: 1.0242 | Train Acc: 0.6182\n",
            "Val Loss: 1.0345 | Val Acc: 0.6144\n",
            "LR: 0.001000\n",
            "Epoch 18/20:\n",
            "Train Loss: 1.0056 | Train Acc: 0.6240\n",
            "Val Loss: 1.0312 | Val Acc: 0.6160\n",
            "LR: 0.001000\n",
            "Epoch 19/20:\n",
            "Train Loss: 0.9884 | Train Acc: 0.6322\n",
            "Val Loss: 1.0328 | Val Acc: 0.6160\n",
            "LR: 0.001000\n",
            "Epoch 20/20:\n",
            "Train Loss: 0.9738 | Train Acc: 0.6399\n",
            "Val Loss: 1.0332 | Val Acc: 0.6152\n",
            "LR: 0.000500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test evaluation\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze()\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "test_acc = evaluate(trained_model, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "training_history['test_acc'] = test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSaDY6lBP80B",
        "outputId": "6ca82ef2-26b4-4b60-df1a-28b94f0decf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVbXDVH4P8xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fm6HQ8VHP8vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPxGsrwSP8oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxvIbvvMP8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ZFOGQ-ToFv2y",
        "outputId": "4d63fbff-f91c-4b51-b284-d731dc7cd100"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabarb2022\u001b[0m (\u001b[33mabarb2022-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Initialize wandb with custom config\"\"\"\n",
        "wandb.init(\n",
        "    project=\"fer-improved-model\",\n",
        "    tags=[\"CNN\", \"improved\", \"FER2013\"],\n",
        "    notes=\"Improved FER model with better regularization\"\n",
        ")\n",
        "print(f\"View run at: {wandb.run.url}\")\n",
        "\n",
        "\n",
        "\"\"\"Log all training artifacts to wandb\"\"\"\n",
        "\n",
        "\n",
        "# 2. Log training history (epoch-level)\n",
        "for epoch in range(len(training_history['epoch'])):\n",
        "    wandb.log({\n",
        "        'epoch': training_history['epoch'][epoch],\n",
        "        'train_loss': training_history['train_loss'][epoch],\n",
        "        'train_acc': training_history['train_acc'][epoch],\n",
        "        'val_loss': training_history['val_loss'][epoch],\n",
        "        'val_acc': training_history['val_acc'][epoch]\n",
        "    }, commit=True)\n",
        "\n",
        "# 3. Log batch metrics (sampled)\n",
        "if 'batch' in training_history:\n",
        "    batch_indices = np.linspace(0, len(training_history['batch'])-1, 1000, dtype=int)\n",
        "    for idx in batch_indices:\n",
        "        wandb.log({\n",
        "            'batch': training_history['batch'][idx],\n",
        "            'train_batch_loss': training_history['train_batch_loss'][idx],\n",
        "            'train_batch_acc': training_history['train_batch_acc'][idx]\n",
        "        }, commit=False)\n",
        "\n",
        "# 4. Log confusion matrix\n",
        "if 'confusion_matrix' in training_history:\n",
        "    cm = training_history['confusion_matrix']\n",
        "    wandb.log({\n",
        "        \"conf_mat\": wandb.plot.confusion_matrix(\n",
        "            y_true=cm['true'],\n",
        "            preds=cm['pred'],\n",
        "            class_names=cm['class_names']\n",
        "        )\n",
        "    })\n",
        "\n",
        "# 5. Log model architecture\n",
        "wandb.watch(model, log='all', log_freq=100, log_graph=True)\n",
        "\n",
        "# 6. Save and log model\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "wandb.save('model_weights.pth')\n",
        "\n",
        "# 7. Log final metrics\n",
        "wandb.summary.update({\n",
        "    'best_val_acc': max(training_history['val_acc']),\n",
        "    'final_train_acc': training_history['train_acc'][-1],\n",
        "    'test_acc': training_history.get('test_acc', 0)\n",
        "})\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "9Is2LpfJFvqy",
        "outputId": "d98d91d9-c7eb-4a47-b15d-f7786280884b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250607_165659-rfpzxwbe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe' target=\"_blank\">radiant-haze-2</a></strong> to <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View run at: https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▁▃▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_batch_acc</td><td>▁</td></tr><tr><td>train_batch_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>██▆▅▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▄▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>8931</td></tr><tr><td>best_val_acc</td><td>0.618</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>final_train_acc</td><td>0.63987</td></tr><tr><td>test_acc</td><td>0.62079</td></tr><tr><td>train_acc</td><td>0.63987</td></tr><tr><td>train_batch_acc</td><td>0.67188</td></tr><tr><td>train_batch_loss</td><td>0.88047</td></tr><tr><td>train_loss</td><td>0.97376</td></tr><tr><td>val_acc</td><td>0.61521</td></tr><tr><td>val_loss</td><td>1.03324</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">radiant-haze-2</strong> at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model/runs/rfpzxwbe</a><br> View project at: <a href='https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model' target=\"_blank\">https://wandb.ai/abarb2022-free-university-of-tbilisi-/fer-improved-model</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_165659-rfpzxwbe/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTHl-_tnUwAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}